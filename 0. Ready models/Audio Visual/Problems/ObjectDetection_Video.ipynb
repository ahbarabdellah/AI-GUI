{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Object Detection using OpenCV"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This Code Template is for Object Detection through webcam video capture using OpenCV library in python. Object Detection is A computer vision technique that deals with detecting object/s (face, eye, any inanimate object) in an image or video. This technique draws boundary or a bounding box around target object and may also include their target label. It has many real-life applications like image retrieval and video surveillance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Required Packages**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install opencv-python"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import imutils\r\n",
    "from imutils.video import VideoStream\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Initialization**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Image Labels\n",
    "\n",
    "Here, the COCO dataset is used for image labeling since the model YOLO is trained on it. This dataset is popular for Object Detection and constitutes 80 labels including oven, toaster, bench, car, etc.\n",
    "\n",
    "The file is downloadable at [coco.names](https://opencv-tutorial.readthedocs.io/en/latest/_downloads/a9fb13cbea0745f3d11da9017d1b8467/coco.names)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "classFile = '' # Path to labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "classes = []\r\n",
    "with open(classFile, 'rt') as f:\r\n",
    "    classes = f.read().rstrip('\\n').split('\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model \n",
    "OpenCV uses the function <Code>cv2.dnn.readNet()</Code> to load a pre-trained weights and network configuration of supported format and build object detection model. This function automatically detects an origin framework of trained model and calls an appropriate function such readNetFromCaffe, readNetFromTensorflow, readNetFromTorch or readNetFromDarknet. \n",
    "\n",
    "#### Model Tuning Parameters\n",
    "1. **model**: const String &  \t\n",
    ">Binary file contains trained weights. \n",
    "\n",
    "2. **config**: const String &\n",
    ">Text file contains network configuration\n",
    "\n",
    "3. **framework**: const String & \n",
    ">Explicit framework name tag to determine a format.\n",
    "\n",
    "More  details at the [API](https://docs.opencv.org/4.5.1/d6/d0f/group__dnn.html)\n",
    "\n",
    "#### YOLO\n",
    "In this tutorial, the model used is YOLO â€” You Only Look Once. It is an extremely fast multi object detection algorithm which uses convolutional neural network (CNN) to detect and identify objects.\n",
    "\n",
    ">**config_path**: [YOLO Configuration File](https://opencv-tutorial.readthedocs.io/en/latest/_downloads/10e685aad953495a95c17bfecd1649e5/yolov3.cfg)\n",
    "\n",
    ">**weights_path**: [YOLO Weights](https://pjreddie.com/media/files/yolov3.weights)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Paths to configuration and weight files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "config_path = '' # configuration path\r\n",
    "weights_path = '' # weights path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "net = cv2.dnn.readNet(config_path, weights_path)\r\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\r\n",
    "\r\n",
    "# determine the output layer\r\n",
    "layer_names = net.getLayerNames()\r\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BLOB\n",
    "OpenCv object detection models take input images as BLOBs. A BLOB is a binary large object (BLOB) is a collection of binary data stored as a single entity.\n",
    "\n",
    "<Code>cv2.dnn.blobFromImage()</Code> \n",
    "creates 4-dimensional blob from image. Optionally resizes and crops image from center, subtract mean values, scales values by scalefactor, swap Blue and Red channels.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "1. **image**: InputArray \t\n",
    ">input image (with 1-, 3- or 4-channels).\n",
    "\n",
    "2. **size**: const Size &\n",
    ">spatial size for output image\n",
    "\n",
    "\n",
    "3. **scalefactor**:\tdouble \n",
    ">multiplier for image values.\n",
    "\n",
    "4. **swapRB**: bool \n",
    ">flag which indicates that swap first and last channels in 3-channel image is necessary.\n",
    "\n",
    "5. **crop**: bool \t\n",
    ">flag which indicates whether image will be cropped after resize or not\n",
    "\n",
    "\n",
    "More details at the [API](https://docs.opencv.org/4.5.1/d6/d0f/group__dnn.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Inference**\r\n",
    "The input video stream is captured through webcam and each frame is processed through the Following code section [[Reference](https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html)] and draws bounding box and a corresponding confidence score around an object. Confidence score is the probability that a bounding box contains an object. \r\n",
    "\r\n",
    "<Code>cv.dnn.NMSBoxes</Code> performs non maximum suppression given boxes and corresponding scores.\r\n",
    "\r\n",
    "#### Parameters\r\n",
    "1. **bboxes**: const std::vector< Rect > &\r\n",
    ">a set of bounding boxes to apply NMS.\r\n",
    "2. **scores**: \tconst std::vector< float > & \r\n",
    ">a set of corresponding confidences.\r\n",
    "3. **score_threshold**: const float \r\n",
    ">a threshold used to filter boxes by score.\r\n",
    "4. **nms_threshold**: \tconst float \r\n",
    ">a threshold used in non maximum suppression.\r\n",
    "\r\n",
    "More details at the [API](https://docs.opencv.org/4.5.1/d6/d0f/group__dnn.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# initialize the video stream and allow the camera sensor to warm up\r\n",
    "print(\"[INFO] starting video stream...\")\r\n",
    "vs = VideoStream(src=0).start()\r\n",
    "time.sleep(2.0)\r\n",
    "\r\n",
    "while True:\r\n",
    "    # grab the frame from the threaded video stream and resize it\r\n",
    "    # to have a maximum width of 400 pixels\r\n",
    "    frame = vs.read()\r\n",
    "    frame = imutils.resize(frame, width=400)\r\n",
    "\r\n",
    "    boxes = []\r\n",
    "    confidences = []\r\n",
    "    classIDs = []\r\n",
    "\r\n",
    "    # grab the frame dimensions and convert it to a blob\r\n",
    "    (h, w) = frame.shape[:2]\r\n",
    "\r\n",
    "    blob = cv2.dnn.blobFromImage(image = frame, \r\n",
    "                             scalefactor = 1/255.0, \r\n",
    "                             size = (256, 256), \r\n",
    "                             swapRB=True, \r\n",
    "                             crop=False)\r\n",
    "\r\n",
    "    # pass the blob through the network and obtain the detections and\r\n",
    "    # predictions\r\n",
    "    net.setInput(blob)\r\n",
    "    outputs = net.forward(output_layers)\r\n",
    "\r\n",
    "    # random colors for bounding box\r\n",
    "    colors = np.random.randint(0, 255, size=(len(classes), 3), dtype='uint8') #np.full((len(classes), 3), 255, dtype='uint8')\r\n",
    "\r\n",
    "    # Bounding Box and Confidence Score\r\n",
    "    for output in outputs:\r\n",
    "        for detection in output:\r\n",
    "            scores = detection[5:]\r\n",
    "            classID = np.argmax(scores)\r\n",
    "            confidence = scores[classID]\r\n",
    "            if confidence > 0.5:\r\n",
    "                box = detection[:4] * np.array([w, h, w, h])\r\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\r\n",
    "                x = int(centerX - (width / 2))\r\n",
    "                y = int(centerY - (height / 2))\r\n",
    "                box = [x, y, int(width), int(height)]\r\n",
    "                boxes.append(box)\r\n",
    "                confidences.append(float(confidence))\r\n",
    "                classIDs.append(classID)\r\n",
    "\r\n",
    "    # Non Maximum Suppression \r\n",
    "    indices = cv2.dnn.NMSBoxes(bboxes = boxes, \r\n",
    "                            scores = confidences, \r\n",
    "                            score_threshold = 0.4, \r\n",
    "                            nms_threshold  = 0.3)\r\n",
    "    if len(indices) > 0:\r\n",
    "        for i in indices.flatten():\r\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\r\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\r\n",
    "            color = [int(c) for c in colors[classIDs[i]]]\r\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\r\n",
    "            text = \"{}: {:.4f}\".format(classes[classIDs[i]], confidences[i])\r\n",
    "            cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_DUPLEX, 0.7, color, 2)\r\n",
    "\r\n",
    "    # show the output frame\r\n",
    "    cv2.imshow(\"Frame\", frame)\r\n",
    "    key = cv2.waitKey(1) & 0xFF\r\n",
    "\r\n",
    "    # if the `q` key was pressed, break from the loop\r\n",
    "    if key == ord(\"q\"):\r\n",
    "        break\r\n",
    "# do a bit of cleanup\r\n",
    "cv2.destroyAllWindows()\r\n",
    "vs.stop()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creator: Vamsi Mukkamala , Github: [Profile](https://github.com/vmc99)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "032a1289d4253b3d0e4373d7e3255421c0caa148fd7c9225f671ef6c33ece4a8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}