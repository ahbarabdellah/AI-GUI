{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duWPWgPsdhUu"
   },
   "source": [
    "# Fill-Mask using RoBERTa Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Code Template is to perform Mask filling operation in python using HuggingFace library. In this template RoBERTa Model is utilized.RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fFR9LMeDv7y"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8o7AEzWoEPeS"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FwilAgkdyqI"
   },
   "source": [
    "### RoBERTA\n",
    "\n",
    "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\n",
    "\n",
    "More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262,
     "referenced_widgets": [
      "8bf396b898e24a2f945efe3807de8dd7",
      "c230175a5b684c57a0c55a47b8b6cdf7",
      "5b9b62d0841b429f83839baafff293e5",
      "6065b893d74043628aa321a77b8cb37d",
      "e0e4622086d047ff902f136138d8731f",
      "33ce2690901a413a9f035612737d2088",
      "2ca3ac29a8164f4691b9b42e72ff47ba",
      "694186e3834945e4b3f310f41a158240",
      "b6c48089794c49f18330b53559851152",
      "1cc4695c06c94e2485d10cbd1134b629",
      "2a3e6a621cac46239527ae511416fae2",
      "fc376a8e0fba41d78caf1fe25a0af2fd",
      "12472e3da45e48e4b4e62f028ae15ee4",
      "aa435f5cbcd2468ab4a3416c5faf5819",
      "f42524440b9b41a895abbcbe2749cd95",
      "bec577df41f94994816ff3bd27ff29c4",
      "59cec1adba33489bba291171a59fd10f",
      "0cc6d837b5484c1a9989944dc12cee2f",
      "b03c331c28c64429bcc51455071a7444",
      "225a10e2bfe942afb26eeeba7dd94454",
      "7fbbc8541c914110a9822597aa6ab665",
      "a224dcffb8c9478798893418dd9dbe9e",
      "b3bed7cbabe548de8954bd3f77af364f",
      "48caaffde165444e81309c8660c8fe3a",
      "1c5f9c4883f14866b4202e75da182335",
      "d57408c8f91e4d14bc029ded34f7b09b",
      "ecfe3450dc00439d9dd6014dfbdb9e0b",
      "4b226dc9b3ac4d4a9812b2cb4dc97d76",
      "b3ffab7178464727b68fbbc122d0d7aa",
      "1038acdf371e48519a13243cdaba7c51",
      "ef630dd7ccd44b2c87c2c4a41cbff2c0",
      "3b507fb81a7643a7aa0f9a40e5332746",
      "4aaace3540d44d4b9295581275444595",
      "0ddd7c3d124f4f4c8a920be370fe8437",
      "045a0178497d4041b72849d4c64bb01c",
      "d153a40400d449269d37d92f610a4f39",
      "ebaab8c08ccf425db0a565e49368ebbf",
      "1394c44fb4284689946bdbdcc2cdcb31",
      "c631f9adb9554a068f5febe023aafbb1",
      "23a26795774246cb88a515d0c0b13946"
     ]
    },
    "id": "mGJLtvlNER7p",
    "outputId": "8865af79-8823-424d-a273-b8afcc180cc9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de58155f8849443495ea9eb7cc2577f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54835767b7c406aa92f4e77288a0640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/657M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cee1896e24d412d92c002dbe7d435e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18863b033d2c4ea7b720d45e003a8a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb99e43225e422c95cba07a6c3d2984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-4zGKpmgu4a"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Fill Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIudVNThEVYR",
    "outputId": "cdf40cee-db2d-4b35-ef96-1fc1974e0a6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'Elon Musk is the founder of Tesla.',\n",
       "  'score': 0.7713844180107117,\n",
       "  'token': 4919,\n",
       "  'token_str': ' Tesla'},\n",
       " {'sequence': 'Elon Musk is the founder of SpaceX.',\n",
       "  'score': 0.2232571840286255,\n",
       "  'token': 14973,\n",
       "  'token_str': ' SpaceX'},\n",
       " {'sequence': 'Elon Musk is the founder of PayPal.',\n",
       "  'score': 0.003995120991021395,\n",
       "  'token': 19804,\n",
       "  'token_str': ' PayPal'},\n",
       " {'sequence': 'Elon Musk is the founder of Twitter.',\n",
       "  'score': 0.0005120498826727271,\n",
       "  'token': 599,\n",
       "  'token_str': ' Twitter'},\n",
       " {'sequence': 'Elon Musk is the founder of Apple.',\n",
       "  'score': 0.0002608041977509856,\n",
       "  'token': 1257,\n",
       "  'token_str': ' Apple'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Elon Musk is the founder of <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creator: Nikhil Shrotri , Github: [Profile](https://github.com/nikhilshrotri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
